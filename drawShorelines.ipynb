{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e736d1d-fc10-45ab-9257-44eea676736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "site = lambda state,start,stop : [f\"usa_{state}_{i:04}\" for i in range(start,stop+1)]\n",
    "\n",
    "# Construct a dict of shoreline names to site ids\n",
    "sitesD = {\n",
    "'Virginia Beach': site('VA',4,4),\n",
    "## Rudee inlet (Jettied)\n",
    "'Duck': site('VA',1,3) + site('NC',39,49),\n",
    "## Oregon Inlet\n",
    "# 'Rodanthe': site('NC',32,38),\n",
    "# Connected at Cape Hatteras\n",
    "'Hatteras': site('NC',30,38),#31),\n",
    "## Hatteras Inlet\n",
    "'Ocracoke Island': site('NC',28,29),\n",
    "## Ocracoke Inlet\n",
    "'Portsmouth Island': site('NC',26,27),\n",
    "# Connected at New Drum Inlet\n",
    "'Dump Island': site('NC',25,25),\n",
    "# Connected at Drum Inlet\n",
    "'Cape Lookout': site('NC',22,24), \n",
    "## Barden Inlet\n",
    "'Shackleford Banks': site('NC',21,21),\n",
    "## Beaufort Inlet\n",
    "'Atlantic Beach': site('NC',19,20), # 19 is Emerald Isle\n",
    "## Bogue Inlet\n",
    "'Bear Island': site('NC',18,18), # River at Bear Inlet?\n",
    "## Browns Inlet\n",
    "'Camp Lejeune': site('NC',17,17),\n",
    "## New River Inlet\n",
    "'Topsail Beach': site('NC',14,16),\n",
    "## New Topsail Inlet?\n",
    "'Lea-Hulaff Island': site('NC',13,13),\n",
    "## Rich Inlet\n",
    "'Figure Eight Island': site('NC',12,12),\n",
    "## Mason Inlet?\n",
    "'Wrightsville Beach': site('NC',11,11),\n",
    "## Masonboro Inlet\n",
    "'Masonboro Island': site('NC',10,10),\n",
    "## Carolina Beach Inlet\n",
    "# 'Carolina Beach': site('NC',7,9),\n",
    "# Connected at Cape Fear\n",
    "# 'Bald Head Island': site('NC',6,6),\n",
    "'Cape Fear': site('NC',6,9),\n",
    "## Cape Fear River?\n",
    "'Oak Island': site('NC',4,5),\n",
    "# Lockwoods Folly Inlet\n",
    "'Holden Beach': site('NC',3,3),\n",
    "# Tubbs Inlet\n",
    "'Ocean Isle Beach': site('NC',2,2),\n",
    "# Shallotte Inlet - jetty near south spit\n",
    "'Sunset Beach': site('NC',1,1),\n",
    "# Little River Inlet - jettied\n",
    "'Watles Island': site('SC',28,28),\n",
    "# Hog Inlet\n",
    "'Myrtle Beach': site('SC',25,27), # lots of storm drains onto beach\n",
    "# Murrells Inlet\n",
    "'Litchfield Beach': site('SC',24,24),\n",
    "# Midway Inlet\n",
    "'Pawleys Island': site('SC',23,23),\n",
    "# Pawleys Inlet - jettied -spit elongates tidal river w/o bay, transects are wrong\n",
    "'Debidue Island': site('SC',22,22),\n",
    "# North Inlet\n",
    "'North Island': site('SC',21,21),\n",
    "# Winyah Bay?\n",
    "}\n",
    "allSites = [x for xs in sitesD.values() for x in xs]#site('SC',21,28)+site('NC',1,49)+site('VA',1,4)\n",
    "slNamesD = {v:k for k in sitesD.keys() for v in sitesD[k]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58774e6-2d2e-4949-920e-7031ac8a98a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "def mylowess(targetTs,col,ts,window=4,it=3): # smoothing window in years\n",
    "    fracNull = (len(col)-col.isnull().sum())/len(col)\n",
    "        # seconds in a solar year\n",
    "    frac = ((86400*365.24219*window)/(max(ts)-min(ts))) * fracNull\n",
    "    return lowess(col,ts,frac,it,xvals=targetTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e75ba2-f21c-4296-8e8b-b0563c20b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# newDates = pd.to_datetime(pd.date_range(start='1984', end='2026', freq='ME'),utc=True)\n",
    "targetDates = pd.to_datetime(pd.date_range(start='1984', end='2026', freq='YE-JUN'),utc=True)\n",
    "# There isn't anything visibly consistent about shorelines sampled more than yearly\n",
    "# Higher sampling than twice a year doesn't work for 1984\n",
    "# targetDates = pd.to_datetime(pd.date_range(start='3-1984', end='2026', freq='6ME'),utc=True)\n",
    "targetTs = [date.timestamp() for date in targetDates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0cea2-1118-4dc1-bf17-8571c9797de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save=True#False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c41cf-676b-4b95-bae1-7bff12059186",
   "metadata": {},
   "source": [
    "### New Idea:\n",
    "- pandas interpolate followed by pandas.rolling().mean()\n",
    "- not sure if I need to interpolate\n",
    "- what if I insert regularly spaced dates and interpolate those?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf12f3-c62c-494b-9f64-8fe364c3e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "# from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "#!wget 'https://zenodo.org/records/15626280/files/shoreline_data.zip?download=1'\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile('shoreline_data.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('shoreline_data')\n",
    "# import os\n",
    "# os.chdir('shoreline_data')\n",
    "\n",
    "def getOffsets(siteID,smooth=False,returnAll=False):\n",
    "    # print(siteID+' ')\n",
    "    df = pd.read_csv(f\"{siteID}/time_series_tidally_corrected.csv\").iloc[:,1:]\n",
    "    # ts = df['dates'].apply(lambda x: pd.Timestamp(x).timestamp())\n",
    "    df['dates'] = pd.to_datetime(df['dates'])\n",
    "    df = df.set_index('dates')\n",
    "    if smooth!='lowess':\n",
    "        df = df.reindex(df.index.union(targetDates)).sort_index()\n",
    "        df = df.interpolate(method='time')\n",
    "    if smooth=='roll':\n",
    "        df = df.rolling(f\"{0.5*365.24219}D\",min_periods=1,center=True).mean()\n",
    "    if returnAll:\n",
    "        return df\n",
    "    # TODO: break dates at replenishment discontinuities and fit each separately\n",
    "    if smooth=='lowess':\n",
    "        print(siteID+' ')\n",
    "        ts = [t.timestamp() for t in df.index]\n",
    "        siteOffsets = df.apply(func=lambda col : mylowess(targetTs,col,ts)) # Takes 6.2 seconds with custom frac\n",
    "        # siteOffsets = df.apply(func=lambda col : lowess(col,ts,0.05,xvals=targetTs)) # Takes 6.0s; 0.08 is mean of custom frac for usa_NC_0030; ranges between 0.05 and 0.09\n",
    "        siteOffsets.index = targetDates#.strftime('%Y-%m-%d')\n",
    "        siteOffsets = siteOffsets.T\n",
    "    else:\n",
    "        siteOffsets = df.loc[targetDates].T\n",
    "    siteOffsets['shoreline'] = slNamesD[siteID]\n",
    "    return siteOffsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de169c8f-cf4e-4024-953f-feca56da4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProcessPoolExecutor() as executor:\n",
    "    offsets = executor.map(getOffsets, allSites,['roll']*len(allSites))\n",
    "offsets = pd.concat(offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6926b9-838e-472f-bacd-205f231ea1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# load the transects\n",
    "#!wget 'https://zenodo.org/records/15626280/files/US_East_transects.geojson?download=1'\n",
    "transects = gpd.read_file('US_East_transects.geojson')\n",
    "# transects = transects.to_crs(transects.estimate_utm_crs())\n",
    "transects = transects[transects['site_id'].isin(allSites)].to_crs(\"EPSG:5070\")\n",
    "transects = transects.join(offsets,on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ab95e-ffbd-472d-ae6c-62837a6c646c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reset line startpoints and offsets to make negative distances go away\n",
    "# geopandas interpolate measures negative distances from endpoints instead of backwatds from startpoints\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "def extend_line_start(geom):\n",
    "    \"\"\"Extend a 2-point line in both directions by a given distance.\"\"\"\n",
    "    p1,p2 = geom.coords\n",
    "    dx,dy = [p2[i] - p1[i] for i in range(2)]\n",
    "    new_p1 = (p1[0] - (dx/geom.length) * geom.length, p1[1] - (dy/geom.length) * geom.length)\n",
    "    return LineString([new_p1, p2])\n",
    "\n",
    "cols = offsets.columns[offsets.dtypes=='float64']\n",
    "transects[cols] = transects[cols].add(transects.geometry.length, axis=0)\n",
    "transects.geometry = [ extend_line_start(g) for g in transects.geometry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f98ed7-7920-41e0-ba13-3d4baa3b9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "def save_gj(gdf,name,gz=True):\n",
    "    gdf.columns = gdf.columns.map(str)\n",
    "    if gz:\n",
    "        with gzip.open(name+'.geojson.gz', 'wt') as f:\n",
    "            f.write(gdf.to_json())\n",
    "    else:\n",
    "        gdf.to_file(name+'.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168b137-8ed3-4b2e-a552-ecd9aa2c9f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_gjgz(filename):\n",
    "    return gpd.GeoDataFrame.from_features(json.load(gzip.open(filename, 'rt'))['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd0ff8c-afa8-487c-ad3b-130270758743",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save:\n",
    "    print('Saving...')\n",
    "    save_gj(transects.copy(),'transects')\n",
    "    print('Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680eea72-76c2-4c06-96ff-c198e72a0fe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import LineString\n",
    "\n",
    "def interpolate_group(group):\n",
    "    geoms = [group[1].geometry.interpolate(group[1][targetDate]) for targetDate in targetDates]\n",
    "    # Remove empty points (where shoreline does not cross transect)\n",
    "    geoms = [g[~(g.is_empty | g.isna())] for g in geoms]\n",
    "    return group[0], [LineString(g) for g in geoms]\n",
    "\n",
    "# # Testing for empty points on Cape Lookout\n",
    "# import numpy as np\n",
    "# print(offsets.iloc[np.where(offsets.isnull())].drop_duplicates())\n",
    "# items =[f\"{ss[:6]}_0022{ss[-5:]}\" for ss in site('NC',74,82)]\n",
    "# interpolate_group(transects.set_index('id').filter(items=items,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156edde-149c-423c-8a3f-05e21d9d14e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# slsD = dict(list(transects.groupby('shoreline').apply(interpolate_group,include_groups=False)))\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    slsD = executor.map(interpolate_group,transects.groupby('shoreline'))\n",
    "slsD = dict(list(slsD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd6493-5cfd-4151-b81f-21b05d343440",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdD = dict(zip(targetDates.strftime('%Y-%m-%d'),zip(*slsD.values())))\n",
    "\n",
    "gdfs = {targetDate: gpd.GeoDataFrame(crs = \"EPSG:5070\", data = \n",
    "    {\n",
    "    'Name' : slsD.keys(),\n",
    "    'site_id' : [sitesD[name] for name in slsD.keys()],\n",
    "    'date' : targetDate,\n",
    "    'geometry' : tdD[targetDate]\n",
    "    })\n",
    "     for targetDate in targetDates.strftime('%Y-%m-%d')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db02d7-620f-4ce6-b62d-3aad45f7cbdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if save:\n",
    "    for targetDate in targetDates.strftime('%Y-%m-%d'):\n",
    "        gdfs[targetDate].to_file('shorelines.gpkg',layer=targetDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc2d62-04e2-4fae-bd87-32f3062df2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "m = folium.Map()\n",
    "# m = transects.iloc[:,:9].explore(name='transects',show=False)\n",
    "for targetDate in targetDates.strftime('%Y-%m-%d')[:4]:\n",
    "    gdfs[targetDate].explore(m=m,name=targetDate)\n",
    "for targetDate in targetDates.strftime('%Y-%m-%d')[4:]:\n",
    "    gdfs[targetDate].explore(m=m,name=targetDate,show=False)\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2226517d-5c1e-47a1-a2fa-46f6620159be",
   "metadata": {},
   "source": [
    "import folium\n",
    "m2 = folium.Map()\n",
    "transects[transects['site_id'].isin(sitesD['Cape Lookout'])].iloc[:,:9].explore(m=m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1f705-70d8-4cff-85ff-3831dd6c4d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b65e53-2bee-45dc-9002-27da370ce4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypymt-cem",
   "language": "python",
   "name": "mypymt-cem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
